---
title: Linear types for programmers
subtitle: reasoning about resources
date: 2023-01-16
tags: Types, Syntax, Semantics, Formal Verification, Programming Languages
---
:source-language: rust

== Introduction

[quote, not Albert Einstein]
You do not really understand something unless you can explain it to
your grandmother.

In this ‘_X_ for programmers’ series I aim to provide quick
introductions — and, more importantly, motivations — for a variety of
technologies and concepts that are well-known in the literature, but
maybe not so familiar to the lay programmer.

I also hope this will serve as a good on-ramp for me to start writing
more.

In this article, I hope to explain how linear logic can be applied to
programming, why it's worthwhile, and give computational
interpretations of the linear logic connectives using Rust.

== What are linear types?

Linear types are an application to type theory of the discipline of
_linear logic_, first described by Jean-Yves Girard
(link:http://girard.perso.math.cnrs.fr/linear.pdf[Girard, 1987]).
Linear types are a refinement of standard type theory in which each
value can be used, and must be used, exactly once, and popularized by
Henry Baker in a series of articles about applying them to remove
garbage collection from Lisp. footnote:[Henry Baker is a prominent
Lisp hacker and one of the original creators of the Lisp Machine,
which is sometimes considered to have lost to the C-based UNIX
machines due to the long and unpredictable ‘pause’ required for
garbage collection on the relatively slow hardware of the time.]

== Why are linear types interesting?

Linear types allow the programmer to model systems in which using
something changes its type.  A common example involves a ‘cost’: if I
have a vending machine that will give me either a chocolate bar or a
packet of crisps for a dollar, I cannot simply put the dollar in twice
and get both.  Using the dollar for one item _consumes_ the dollar,
and it is no longer available to get the other item.

A more usual example in everyday programming involves objects that
have certain protocols (a sequencing of the available operations) that
must be obeyed.  For example, once a file handle has been closed, it
can no longer be written to: from the programmer's perspective, the
file handle no longer exists.  More sophisticated changes to state,
for example an ‘unopened’ file handle whose metadata can be accessed
becoming an ‘opened’ file handle to/from which the programmer can
write/read data, can be modelled as ‘destroying’ the old object and
returning a new one.

A related application is that of a network protocol.  For example, on
a stream implementing HTTP, the server might expect the client to:

. open a socket
. send headers
. write a request body
. read a response

Any misordering of these operations, for example writing the request
body before the headers, constitutes an error on the part of the
client programmer.

Another very important example is that of memory management.  Because
arbitrary duplication of values is disallowed, it is inherent in the
structure of the code exactly how long a value must stay around, and
therefore when it is safe to reuse its memory.  This allows linear
languages, in general, to be safely executed without recourse to a
garbage collector, reference counter, or other means of tracking
memory references.

== How are linear types used when programming?

In its most basic form, linear logic replaces the standard conjunction
and disjunction connectives of intuitionistic or classical logic
(∧/_and_ and ∨/_or_) each with an _additive_ and _multiplicative_
version.

- _A_ ⊗ _B_, read _A times B_, is the multiplicative conjunction;
- _A_ ⅋ _B_, read _A par B_, is the multiplicative disjunction;
- _A_ ⊕ _B_, read _A plus B_, is the additive disjunction;
- _A_ & _B_, read _A with B_, is the additive conjunction.

I find the semantics of these connectives easiest to describe in terms
of computational interpretation.  Additive connectives indicate that
the computation branches: we take _either_ the right path _or_ the
left path, as decided by either the constructor of the value or the
consumer.  Multiplicatives, however, indicate that _both_ branches of
the computation proceed.

== The _times_ and _plus_

The times and the plus are familiar to programmers used to a modern
programming language.  In Rust, for example, the times _A_ ⊗ _B_
corresponds to the familiar `struct` type former:

----
struct Times<A, B> {
  left:  A,
  right: B,
}
----

while the plus _A_ ⊕ _B_ corresponds to the `enum` type former:

----
enum Plus<A, B> {
  Left (A),
  Right(B),
}
----

Before we get onto the less comfortable examples, it's instructive to
pause here and look at what this means for the code _consuming_ the
value.  If some code has some `Times<A, B>` in scope,

----
let atimesb: Times<A, B>
  = make_ATimesB();
…
----

it can access _both_ an `A` (called `atimesb.left`) _and_ a `B`
(called `atimesb.right`); and, conversely, `make_ATimesB` has the
responsibility to create both an `A` _and_ a `B`.

On the other hand, if it has some `Plus<A, B>` in scope,

----
let aplusb: Plus<A, B> = make_APlusB();
…
----

it can access _either_ an `A` _or_ a `B` (by ``match``ing on
`aplusb`), depending on what `make_APlusB` decided to give back:
`make_APlusB` only has the responsibility to create _one of_ an `A` or
a `B`.

== The _with_ and _par_

A less familiar construction is the additive conjunction, _with_ or &.
This operation has no direct counterpart in most programming
languages.  Like the plus, the with connective _A_ & _B_ presents only
one of _A_ or _B_ (it is additive), but now it is the consumer who
gets to choose which one is produced.

In Rust, we might encode this as a type with two functions:

----
impl<A, B> With<A, B> {
  fn left(self) -> A { … }
  fn right(self) -> B { … }
}
----

but there is a subtlety here.

In these examples, we've been working with Rust.  Rust is an _affine_
language by its nature.  An affine language, like a linear language,
forbids arbitrary duplication of values. footnote:affinity[The
difference is that in an affine language we are allowed to _drop_ any
value whenever we please.  In a truly linear language, values are not
only _resources_ that may be consumed by the program to help it
perform its goal, but also _obligations_ that must be discharged: if a
program has a value of type `A` in scope, it _must_ perform an
operation that disposes of the `A`.  This can be used to ensure that,
for example, the program always correctly cleans up any memory it owns
or files it has created, or that it always executes a protocol
correctly through to its end.]  If this were not the case, our `With`
above would be essentially the same as `Times`: we could obtain both
an `A` by calling `awithb.left()` as well as a `B` by calling
`awithb.right()` and thereby write the function:

----
fn with_to_times<A, B>(
  awithb: With<A, B>,
) -> Times<A, B> {
  Times {
    left:  awithb.left(),
    right: awithb.right(),
  }
}
----

In fact, though, we can't necessarily make a copy of `With<A, B>`!
The consumer must _choose_ which of `A` and `B` it wants to proceed
with… which, dually, means that the producer is free to use any
resources available to it to construct the `With<A, B>` in _both_
`left()` _and_ `right()`, since it knows that only one of them will
ever be called and therefore the resources will be used only once.  We
can encode this conveniently in Rust by having both functions consume
`self` — if `self` can't be copied, we are guaranteed that only one of
the two functions can be called for each `With<A, B>`.

'''

Probably the most exotic beast we'll talk about here, but perhaps also
the most interesting, the _par_ operation _A_ ⅋ _B_ is a value that
will produce both an _A_ and a _B_, but gets to decide in what order
they are produced.  Note the duality with the times: whereas the
consumer of a times may consume the values in either order, and the
producer must account for that, with a par the producer may produce
the values in either order, and the consumer must account for that.

One of the most common uses of the par is to express a function: the
function type _A_ ⊸ _B_ is defined as _A_^⊥^ ⅋ _B_, where the notation
_A_^⊥^ indicates the formula that is the negation or opposite of _A_ —
or, equivalently, a continuation that consumes an _A_.  footnote:[The
notation used here, rather than the usual function arrow →, is
actually that of the ‘lollipop,’ a symbol used to indicate the type of
_linear_ functions that consume their argument exactly once.]  This
makes use of a rather fundamental equivalence between _scheduling_ and
_causality_.  A function cannot be expressed by use of the times
because the consumer of the times may access its components in any
order, and therefore the caller can't know that the ‘result’ of the
function won't be expected before its argument has been provided.
However, in the case of the par, because the producer of the par gets
to schedule the execution of both components, it can choose to ensure
that the argument is evaluated before producing a result, and
therefore vary the result it produces based on the behaviour of the
argument.  The easiest way to use this information, and a key example
of a function, is the identity function _A_ ⊸ _A_ ≝ _A_^⊥^ ⅋ _A_,
which can be implemented simply by passing the _A_ provided to the
_A_^⊥^ directly out of the other component.

This one is quite tricky to express in Rust, because it requires us to
think a bit harder about what we mean by a ‘value’.  Notionally, we
would like to be able to write the par as a ‘function’ that can return
multiple results (using the keywords `return_left` and `return_right`
here):

[subs=+quotes]
----
impl<A, B> Par<A, B> {
  fn par(self) -> (A ⅋ B) {
    let a: ⊥<A> = return_left mkA();
    return_right mkB(a);
  }
}
----

Unfortunately, this operation doesn't exist in Rust. footnote:[Indeed,
it can't exist: due to Rust's stack-based semantics, as soon as we
return one result, the stack frame is destroyed, meaning we can no
longer continue running the code to produce the other.  In `async`
functions, this doesn't have to be true – one could return a pair of
`oneshot::Receiver` objects and then fulfil them both in any order
from a newly `spawn`ed task — but Rust provides no way to guarantee
the order of their consumption matches the order of production, which
may lead to deadlocks.]  Instead, we must use a bit of a trick.
_Continuation-passing style_ is a way of writing programs such that
instead of producing return values from our functions, we take as a
function argument a callback, or continuation, indicating how the
function should proceed.  That is, instead of writing

----
fn foo(arg: A) -> B { … }
----

we can instead write

----
fn foo(
  arg:  A,
  cont: impl FnOnce(result: B) -> !,
) -> ! { … }
----

with much the same meaning, passing the ‘continuation’ of the program
(i.e. what is to be done after the value is successfully produced) as
a callback that will be called after the computation is
complete.

[NOTE]
.return values
====

The `!` type in Rust is the empty type, or ‘never type’, which has no
values.  Since it is impossible to construct, its use here means that
we will never return a value.  An equivalent construction would be to
allow these functions to notionally produce a value of _any_ type,

----
fn foo<R>(
  arg:  A,
  cont: impl FnOnce(result: B) -> R,
) -> R { … }
----

since we will never actually be required to construct it.
====

Continuation-passing style can simplify reasoning about control flow,
and is key to some crucial compiler translations — for example, it is
often used to implement non-local control (such as
link:https://en.wikipedia.org/wiki/Exception_handling[exceptions],
link:https://en.wikipedia.org/wiki/Async/await[async/await], or
link:https://en.wikipedia.org/wiki/Effect_system[effects]
a.k.a. link:https://gigamonkeys.com/book/beyond-exception-handling-conditions-and-restarts.html[restarts]).

Armed with this tool, we can define:

----
impl Par<A, B> {
  fn par(
    self,
    left:  impl FnOnce(A) -> !,
    right: impl FnOnce(B) -> !,
  ) -> ! { … }
}
----

Note that, because we must provide _both_ `left` and `right`
continuations, the contexts must be disjoint!  In fact, this yields an
interesting insight into our boring times: notice that this signature
is basically equivalent to

----
impl<A, B> Par<A, B> {
  fn par(
    self,
    continuations: Times<
      impl FnOnce(A) -> !,
      impl FnOnce(B) -> !,
    >,
  ) -> ! { … }
}
----

In fact, _A_ ⊗ _B_ must be interpreted as both _A_ and _B_, but
defined _independently_, i.e. using disjoint contexts and, crucially,
without relying on _A_ and _B_ being executed in a particular order.

== Conclusion and retrospective

Hopefully this has provided an accessible introduction to linear logic
and its interpretation into programming languages in the form of
linear types.

I had hoped for this article to motivate the concepts rather than
explain them, but I fear I have ended up producing more of a technical
explanation of what the connectives of linear logic _are_ in a
computational sense.  I'm not totally dissatisfied with this outcome,
but I hope it will improve in future articles.

Things I'd like to try next:

- more pictures/diagrams
- more focus on motivation and less on explanation
